import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# æ£€æŸ¥è®¾å¤‡ (GPU ä¼˜å…ˆ)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ± è¿è¡Œè®¾å¤‡: {device}")

# ---------------------------------------------------------
# 1. æ•°æ®é›†ç±» (Dataset)
# ---------------------------------------------------------
class TEPDataset(Dataset):
    def __init__(self, X, y):
        # PyTorch Conv1d éœ€è¦è¾“å…¥å½¢çŠ¶ä¸º (Batch, Channels, Length)
        # åŸå§‹ X å½¢çŠ¶: (Sample, 41) -> å˜æˆ (Sample, 1, 41)
        self.X = torch.FloatTensor(X).unsqueeze(1) 
        # æ ‡ç­¾å¦‚æœæ˜¯ CrossEntropyLossï¼Œéœ€è¦ LongTensor ä¸”ä¸éœ€è¦ One-hot (ç›´æ¥ç”¨ 0, 1)
        self.y = torch.LongTensor(y)
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

def load_data():
    """è¯»å–æ•°æ®ï¼Œå¡«å……ç¼ºå¤±å€¼ï¼Œç”Ÿæˆæ ‡ç­¾å¹¶åˆ’åˆ†"""
    try:
        print("ğŸ“¥ æ­£åœ¨è¯»å–æ•°æ®...")
        df = pd.read_csv("./TEP_3000_Block_Split.csv")
        
        # 1. æå–ç‰¹å¾
        data_df = df.filter(like='xmeas_').iloc[:, :41]
        
        # 2. å¡«å…… NaN (ç”¨ 0 å¡«å……ï¼Œå¯¹åº” Mask æœºåˆ¶)
        data_filled = data_df.fillna(0).astype(float).to_numpy()
        
        # 3. ç”Ÿæˆæ ‡ç­¾ (å‰1500æ­£å¸¸=0, å1500å¼‚å¸¸=1)
        # æ³¨æ„ï¼šPyTorch CrossEntropyLoss æœŸæœ›æ ‡ç­¾æ˜¯ 1D æ•°ç»„ [0, 1, 0, ...]ï¼Œè€Œä¸æ˜¯ One-hot
        y = np.concatenate([np.zeros(1500), np.ones(1500)])
        
        print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {data_filled.shape}")
        
        # 4. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            data_filled, y, test_size=0.2, random_state=42, shuffle=True
        )
        
        return X_train, X_test, y_train, y_test
        
    except FileNotFoundError:
        print("æ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
        return None, None, None, None

# ---------------------------------------------------------
# 2. æ¨¡å‹æ„å»º (CNN-LSTM)
# ---------------------------------------------------------
class CNNLSTMModel(nn.Module):
    def __init__(self, input_len=41, num_classes=2):
        super(CNNLSTMModel, self).__init__()
        
        # --- CNN éƒ¨åˆ† (æå–å±€éƒ¨ç‰¹å¾) ---
        # å‚ç…§è®ºæ–‡ Table 5 [cite: 454]ï¼Œä½†ä¸ºäº†é€‚é… input_len=41 å‡å°äº† kernel_size
        
        # Layer 1: Conv -> Tanh -> MaxPool
        # Input: (Batch, 1, 41) -> Padding=2 ä¿æŒé•¿åº¦ -> (Batch, 16, 41)
        self.c1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, padding=2)
        self.act1 = nn.Tanh() # è®ºæ–‡å…¬å¼ (1) æŒ‡å®š Tanh [cite: 143]
        self.p1 = nn.MaxPool1d(kernel_size=2)
        
        # Layer 2
        # Input: (Batch, 16, 20)
        self.c2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)
        self.act2 = nn.Tanh()
        self.p2 = nn.MaxPool1d(2)
        
        # Layer 3
        # Input: (Batch, 32, 10)
        self.c3 = nn.Conv1d(32, 64, kernel_size=3, padding=1)
        self.act3 = nn.Tanh()
        self.p3 = nn.MaxPool1d(2)
        
        # Output after CNN: (Batch, 64, 5)
        
        # --- LSTM éƒ¨åˆ† (æŒ–æ˜æ—¶é—´ç›¸å…³æ€§) ---
        # LSTM éœ€è¦è¾“å…¥ (Batch, Sequence, Features)
        # æ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨ forward ä¸­æŠŠ (Batch, 64, 5) è½¬ç½®ä¸º (Batch, 5, 64)
        self.lstm = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)
        
        # --- å…¨è¿æ¥ä¸åˆ†ç±» ---
        # FC (Nodes 32) -> Tanh -> BN [cite: 250, 251]
        self.fc1 = nn.Linear(64, 32)
        self.fc_act = nn.Tanh()
        self.bn = nn.BatchNorm1d(32)
        
        # Output Layer
        self.fc_out = nn.Linear(32, num_classes)
        # æ³¨æ„ï¼šä¸åŠ  Softmaxï¼Œå› ä¸º CrossEntropyLoss ä¼šè‡ªåŠ¨å¤„ç†

    def forward(self, x):
        # x shape: (Batch, 1, 41)
        
        # CNN Block
        x = self.p1(self.act1(self.c1(x)))
        x = self.p2(self.act2(self.c2(x)))
        x = self.p3(self.act3(self.c3(x)))
        
        # å‡†å¤‡ LSTM è¾“å…¥
        # å½“å‰ shape: (Batch, 64, 5) -> éœ€è¦ (Batch, 5, 64)
        x = x.permute(0, 2, 1)
        
        # LSTM
        # out: (Batch, Seq_Len, Hidden), (h_n, c_n)
        # æˆ‘ä»¬å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä½œä¸ºç‰¹å¾
        out, (h_n, c_n) = self.lstm(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥: h_n[-1] æˆ–è€… out[:, -1, :]
        x = out[:, -1, :] 
        
        # FC -> BN
        x = self.fc_act(self.fc1(x))
        x = self.bn(x)
        
        # Output logits
        x = self.fc_out(x)
        return x

# ---------------------------------------------------------
# 3. è®­ç»ƒä¸éªŒè¯æµç¨‹
# ---------------------------------------------------------
def train_model():
    # 1. å‡†å¤‡æ•°æ®
    X_train, X_test, y_train, y_test = load_data()
    if X_train is None: return
    
    train_dataset = TEPDataset(X_train, y_train)
    test_dataset = TEPDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = CNNLSTMModel().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001) # [cite: 268]
    
    # 3. è®­ç»ƒå¾ªç¯
    epochs = 100
    train_acc_history = []
    test_acc_history = []
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()
            
        train_acc = 100 * correct_train / total_train
        train_acc_history.append(train_acc)
        
        # éªŒè¯
        model.eval()
        correct_test = 0
        total_test = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total_test += labels.size(0)
                correct_test += (predicted == labels).sum().item()
        
        test_acc = 100 * correct_test / total_test
        test_acc_history.append(test_acc)
        
        print(f"Epoch [{epoch+1}/{epochs}] "
              f"Loss: {running_loss/len(train_loader):.4f} | "
              f"Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%")

    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    
    # 4. ç»˜å›¾
    plt.figure(figsize=(10, 5))
    plt.plot(train_acc_history, label='Train Accuracy')
    plt.plot(test_acc_history, label='Test Accuracy')
    plt.title('PyTorch CNN-LSTM Fault Diagnosis')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.show()

if __name__ == "__main__":
    train_model()